import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sb
from keras import layers, models
from sklearn.exceptions import DataConversionWarning
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

warnings.filterwarnings(action = 'ignore', category = DataConversionWarning) 

class Preprocessing_Dataset2:
    def __init__(self):
            #csv = 'D:\OneDrive\Academia\MSc Machine Learning in Science\Modules\COMP3009 Machine Learning\Submissions\Assignment 1\obesity.csv'
            #self.train_X, self.train_Y, self.test_X, self.test_Y = self.data_manager(csv)
            pass

    def data_manager(self, csv):
        pass

class Preprocessing_Wine:
    def __init__(self):
        csv = 'D:\OneDrive\Academia\MSc Machine Learning in Science\Modules\COMP3009 Machine Learning\Submissions\Assignment 1\wine.csv'
        encoder = LabelEncoder()
        self.train_X, self.train_Y, self.test_X, self.test_Y = self.data_manager(csv, encoder)

    def data_manager(self, csv, encoder):
        #import file, drop rows containing N/A
        input = pd.read_csv(csv, header = 0).dropna(axis = 0)
        
        #70% training data, normalise features, split class
        training_data = input.sample(frac = 0.7, random_state = 200)
        train_X = self.normalise(training_data.iloc[:, 0:11])
        train_y = training_data.iloc[:, 11:12]
        
        #30% testing data, normalise features, split class
        testing_data = input.drop(training_data.index)
        test_X = self.normalise(testing_data.iloc[:, 0:11])
        test_y = testing_data.iloc[:, 11:12]

        train_y = encoder.fit_transform(train_y)
        test_y = encoder.fit_transform(test_y)
        train_X.to_numpy()
        test_X.to_numpy()

        return train_X, train_y, test_X, test_y

    def normalise(self, input_features):
        #set column values between 0-1 independently
        x = input_features

        for i in x.columns:
            x[i] = (x[i] - x[i].min()) / (x[i].max() - x[i].min())

        return x

    def correlation(self, csv):
        df = pd.read_csv(csv) 
        correlation_matrix = df.corr()

        # Generate a mask for the upper triangle
        mask = np.zeros_like(correlation_matrix)
        mask[np.triu_indices_from(mask)] = True

        # Set up the matplotlib figure
        fig, ax = plt.subplots(figsize=(20, 12))
        plt.title('Obesity Feature Correlation')

        # Generate a custom diverging colormap
        cmap = sb.diverging_palette(260, 10, as_cmap = True)

        # Draw the heatmap with the mask and correct aspect ratio
        sb.heatmap(correlation_matrix, vmax = 1.2, square = False, cmap = cmap, mask = mask, ax = ax, annot = True, fmt = '.2g', linewidths = 1)

class Classification_MultilayerPerceptron:
    def __init__(self, train_X, train_y, test_X, test_y):
        model_accuracy = self.train(train_X, train_y, test_X, test_y)
        print('\nMultilayer Perceptron Classification Model Accuracy (Per Fold): {}'.format(model_accuracy))
   
    def train(self, train_X, train_y, test_X, test_y):
        inputs = np.concatenate((train_X, test_X), axis = 0)
        targets = np.concatenate((train_y, test_y), axis = 0)
        
        kf = KFold(5, shuffle = True, random_state = 42) 
        fold = 0
        accuracy_per_fold = []
        loss_per_fold = []

        for(train, test) in kf.split(inputs, targets):
            fold += 1
            print('Fold: {}'.format(fold))

            model = models.Sequential([
                                       layers.Dense(10, input_dim = 11, activation = 'relu'),
                                       layers.Dropout(0.1),
                                       layers.Dense(30, activation = 'relu'),
                                       layers.Dropout(0.1),
                                       layers.Dense(6, activation = 'softmax')
            ])

            model.compile(
                          loss = 'sparse_categorical_crossentropy', 
                          optimizer = 'sgd', 
                          metrics = ['accuracy']
            )

            model.fit(inputs[train], targets[train], epochs = 10)
            scores = model.evaluate(inputs[test], targets[test], verbose = 0)
            accuracy_per_fold.append('{}%'.format(np.round(scores[1] * 100, 2)))

        return accuracy_per_fold

class Classification_MultinomialLogisticRegression:
    def __init__(self, train_X, train_y, test_X, test_y):
        model = self.model_init()
        model = self.train(model, train_X, train_y)
        model_accuracy = self.test(model, test_X, test_y)
        print('\nMultinomial Logistic Regression Classification Model Accuracy: {}%'.format(100 * model_accuracy))

    def model_init(self):
        model = LogisticRegressionCV(
                                     max_iter = 2000,
                                     cv = 5,
                                     multi_class = 'multinomial', 
                                     solver = 'lbfgs', 
                                     penalty = 'l2', 
                                     )

        return model

    def train(self, model, train_X, train_y):
        model.fit(train_X, np.ravel(train_y))

        return(model)

    def test(self, model, test_X, test_y):
        pred_y = model.predict(test_X)
        
        return accuracy_score(test_y, pred_y)

class Classification_SVM:
    def __init__(self, train_X, train_y, test_X, test_y):
        k_folds = 5
        score, cross_val = self.train(train_X, train_y, test_X, test_y, k_folds)

        print('\nSupport Vector Machine Classification Model Accuracy: {}%'.format(np.round(score * 100, 2)))
        print('Support Vector Machine Classification {}-Fold Cross-Validation Score: {}%'.format(k_folds, np.round(cross_val * 100, 2)))
    
    def train(self, train_X, train_y, test_X, test_y, folds):
        clf = SVC(C = 1.0, kernel = 'rbf', degree = 3, gamma = 'auto', probability = True)
        clf.fit(train_X, np.ravel(train_y))
        score = clf.score(test_X, test_y)
        cross_val = np.average(cross_val_score(clf, train_X, np.ravel(train_y), cv = folds))

        return score, cross_val
        
class Classification_DecisionTree:
    def __init__(self, train_X, train_y, test_X, test_y):
        k_folds = 5
        score, cross_val = self.train(train_X, train_y, test_X, test_y, k_folds)

        print('\nDecision Tree Classification Model Accuracy {}%'.format(np.round(score * 100, 2)))
        print('Decision Tree Classification Model {}-Fold Cross-Validation Score: {}%'.format(k_folds, np.round(cross_val * 100, 2)))

    def train(self, train_X, train_y, test_X, test_y, folds):
        clf = DecisionTreeClassifier()
        clf.fit(train_X, np.ravel(train_y))
        score = clf.score(test_X, test_y)
        cross_val = np.average(cross_val_score(clf, train_X, np.ravel(train_y), cv = folds))
        
        return score, cross_val
        
if __name__ == '__main__':
    wine_dataset = Preprocessing_Wine()
    #dataset2 = Preprocessing_Dataset2

    Classification_MultilayerPerceptron(wine_dataset.train_X, wine_dataset.train_Y, wine_dataset.test_X, wine_dataset.test_Y)
    Classification_MultinomialLogisticRegression(wine_dataset.train_X, wine_dataset.train_Y, wine_dataset.test_X, wine_dataset.test_Y)
    Classification_SVM(wine_dataset.train_X, wine_dataset.train_Y, wine_dataset.test_X, wine_dataset.test_Y)
    Classification_DecisionTree(wine_dataset.train_X, wine_dataset.train_Y, wine_dataset.test_X, wine_dataset.test_Y)

    #insert regression models
